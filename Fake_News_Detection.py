# -*- coding: utf-8 -*-
"""Mini_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JoQrAGNqGsGy6r5lUBpJqpGSMSTouvUV
"""

# Our toolbox!
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout #



# Step 1: Read both datasets
true_df = pd.read_csv('/content/True.csv')  # Real news
fake_df = pd.read_csv('/content/Fake.csv')  # Fake news

true_df.head()

fake_df.head()

# Step 2: Add labels so the model knows what's real and what's fake
true_df['label'] = 1  # Real news = 1
fake_df['label'] = 0  # Fake news = 0

true_df.columns

fake_df.columns

# Step 3: Combine the good, the bad, into 1 mega one
df = pd.concat([true_df, fake_df])

df.sample(5)

# Step 4: Shuffle it like a playlist
df = df.sample(frac=1).reset_index(drop=True)

# Step 5: Merge title and body into one big story
df['text'] = df['title'] + " " + df['text']

# This function makes our text clean, neat, and ready for LSTM
def clean_text(text):
    text = text.lower() # Make it all lowercase, we don't like uppercasing drama
    text = re.sub(r'\W', ' ', text)   # Remove weird characters: #@$&^% etc.
    text = re.sub(r'\s+', ' ', text) # Remove extra spaces – we like minimalism
    return text

# Combine title and full article into 1
df['text'] = df['title'] + " " + df['text']
df['text'] = df['text'].apply(clean_text)

#  Time to separate inputs (X) and answers (y) like we separate coffee and sugar ☕
X_data = df['text'].values  # These are our inputs: cleaned text
y_data = df['label'].values  # These are our outputs: real or fake

#  WordCloud
!pip install wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Separate the text for each category
true_text = " ".join(df[df['label'] == 1]['text'].tolist())
fake_text = " ".join(df[df['label'] == 0]['text'].tolist())

#  Real news WordCloud – let's see what truth looks like
true_wc = WordCloud(width=1920, height=1000, background_color='white').generate(true_text)

plt.figure(figsize=(10, 10))
plt.imshow(true_wc, interpolation='bilinear')
plt.axis('off')
plt.title("TRUE News")
plt.show()

#  Fake news WordCloud – the world of misinformation 🌪️
fake_wc = WordCloud(width=1920, height=1000, background_color='black', colormap='Reds').generate(fake_text)

plt.figure(figsize=(10, 10))
plt.imshow(fake_wc, interpolation='bilinear')
plt.axis('off')
plt.title("FAKE News")
plt.show()

# Only keep the top 10,000 words – because who remembers more than that?
max_words = 10000
max_len = 200  # Max length of a news article (we'll cut the long ones)

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(df['text']) # Learn the vocabulary

# Convert words to numbers (LSTM only understands in digits)
sequences = tokenizer.texts_to_sequences(df['text'])
X = pad_sequences(sequences, maxlen=max_len) # Make all texts same length
y = df['label'].values  # Real (1) or Fake (0)

# Let's split the news(India and Pakistan)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

model = Sequential()

model.add(Embedding(max_words, 128, input_length=max_len)) # First layer: turn numbers into word meaning vectors

model.add(LSTM(64, return_sequences=False)) # number of lstm cells .

model.add(Dropout(0.5)) # Dropout so the model doesn’t overthink and memorize everything

model.add(Dense(1, activation='sigmoid')) # Final decision: Real or Fake?

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # The loss function that measures how well the model is doing during training.

model.summary()

history = model.fit(X_train, y_train, validation_split=0.2, epochs=5, batch_size=64)
#  training ....

# Final test!
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Plot to show off our model’s learning curve
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""## **Time to Test !!**"""

# Replace this with your own news article to test it!
sample_news = [ "As the National Stock Exchange (NSE) Nifty 50 index quoted around 22,400 levels in opening trades on Wednesday, and in doing so the index is now down for the 65th straight trading session below its long-term 200-day Daily Moving Average (200-DMA). Historical data shows that the Nifty has now spent the longest time below this key moving average since the Covid-19 triggered dramatic collapse. After registering its new peak at 26,277 in late September 2024, the Nifty dropped below the 200-DMA for the first time on November 14, 2024. The index, thereafter, swung around this long-term moving average for nearly two months."]

def clean_text(text):
    text = text.lower()
    text = re.sub(r'\W', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text

cleaned_sample = [clean_text(news) for news in sample_news]

sample_seq = tokenizer.texts_to_sequences(cleaned_sample)
sample_pad = pad_sequences(sample_seq, maxlen=max_len)

prediction = model.predict(sample_pad)
label = ['Fake News 🟥', 'Real News 🟩']

print(f"Prediction Score: {prediction[0][0]:.4f}")
print("Predicted Label:", label[int(prediction[0][0] > 0.5)])